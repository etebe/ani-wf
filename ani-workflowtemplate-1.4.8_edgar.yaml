apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: ani-wft-1.4.8
  generateName: ani-wft-1.4.8-
  namespace: argowf # edit
  labels:
    topic: "ani"
spec:
  serviceAccountName: argo # edit
  parallelism: 2000 # edit
  ttlStrategy:
    secondsAfterCompletion: 86400
    secondsAfterSuccess: 86400
    secondsAfterFailure: 86400
  podGC:
    strategy: OnPodCompletion
  podDisruptionBudget:
    maxUnavailable: 0
  entrypoint: main
  arguments:
    parameters:
      - name: input-file
      - name: id
  volumeClaimTemplates:
    - metadata:
        name: workdir
      spec:
        accessModes:
          - ReadWriteMany
        resources:
          requests:
            storage: 300Gi # edit
        storageClassName: nfs-storage

  templates:
    - name: main
      steps:
        - - name: step1
            template: rename
        - - name: step2
            template: extractHeader
          - name: step3
            template: extractSeqid
          - name: step4
            template: makeCombilist
        - - name: step5
            template: prepareRef
            arguments:
              parameters:
                - name: index
                  value: "{{item}}"
            withSequence:
              count: "{{steps.step1.outputs.parameters.count}}"
          - name: step6
            template: prepareQuery
            arguments:
              parameters:
                - name: index
                  value: "{{item}}"
            withSequence:
              count: "{{steps.step1.outputs.parameters.count}}"
        - - name: step7
            template: calculate
            arguments:
              parameters:
                - name: index
                  value: "{{item}}"
            withSequence:
              count: "{{steps.step4.outputs.parameters.count}}"
        - - name: step8
            template: collect
        - - name: step9
            template: convertToJSON

    - name: rename
      inputs:
        artifacts:
          - name: inputdata
            path: /tmp/inputs
            s3:
              key: data/{{workflow.parameters.input-file}}
      script:
        image: >-
          harbor.computational.bio.uni-giessen.de/docker_hub_cache/library/busybox:1.35
        command: [sh]
        source: |
          mkdir -p /tmp/data
          COUNT=0
          for file in $(find /tmp/inputs -name '*.f*'); do 
            mv $file /tmp/data/${COUNT}.fas
            COUNT=$((COUNT+1))
          done
          echo $COUNT > /tmp/count_data
        volumeMounts:
          - name: workdir
            mountPath: /tmp
        resources:
          requests:
            cpu: "1"
            memory: 6Gi
      outputs:
        parameters:
          - name: count
            valueFrom:
              path: /tmp/count_data

    - name: extractHeader
      script:
        image: >-
          harbor.computational.bio.uni-giessen.de/docker_hub_cache/library/busybox:1.35
        command: [sh]
        source: |
          mkdir -p /tmp/headers
          for file in /tmp/data/*.fas; do
            F=$(basename $file)
            cat $file | head -1 > /tmp/headers/${F}_header
          done
        volumeMounts:
          - name: workdir
            mountPath: /tmp
        resources:
          requests:
            cpu: "1"
            memory: 6Gi

    - name: extractSeqid
      script:
        image: >-
          harbor.computational.bio.uni-giessen.de/docker_hub_cache/library/busybox:1.35
        command: [sh]
        source: |
          mkdir -p /tmp/seqids
          for file in /tmp/data/*.fas; do
            F=$(basename $file)
            cat $file | head -1 | cut -c 2- | tr ' ' '_' | tr ',' '_' > /tmp/seqids/${F}_seqid
          done
        volumeMounts:
          - name: workdir
            mountPath: /tmp
        resources:
          requests:
            cpu: "1"
            memory: 6Gi

    - name: makeCombilist
      script:
        image: >-
          harbor.computational.bio.uni-giessen.de/docker_hub_cache/library/python:3.8-alpine
        command: [python]
        source: | # edit chunk_size
          from itertools import combinations
          import glob
          import math
          import os
          os.mkdir("/tmp/chunks")
          files = glob.glob('/tmp/data/*.fas', recursive=True)
          combi_list = list(combinations(files,2))
          chunk_size = math.ceil(len(combi_list)/2000)
          chunked_list = list()
          for i in range(0, len(combi_list), chunk_size):
            chunked_list.append(combi_list[i:i+chunk_size])
          with open ("/tmp/count_pods", "w") as f:
            f.write(str(len(chunked_list)))
          for i in range(len(chunked_list)):
            with open("/tmp/chunks/chunk_" + str(i), "w") as f:
              f.write(str(chunked_list[i]))
        volumeMounts:
          - name: workdir
            mountPath: /tmp
        resources:
          requests:
            cpu: "1"
            memory: 6Gi
      outputs:
        parameters:
          - name: count
            valueFrom:
              path: /tmp/count_pods

    - name: prepareRef
      inputs:
        parameters:
          - name: index
      script:
        image: >-
          harbor.computational.bio.uni-giessen.de/docker_hub_cache/library/busybox:1.35
        command: [sh]
        source: |
          mkdir -p /tmp/refs
          mkdir /tmp/ref-dir-{{inputs.parameters.index}}
          WD="/tmp/ref-dir-{{inputs.parameters.index}}"
          R="/tmp/data/{{inputs.parameters.index}}.fas"
          F=$(basename $R)
          NS=$(cat $R | grep "^>" | wc -l)
          if [ $NS -gt 1 ]; then
            sed '/^>/d' $R | tr -d '\n' > ${WD}/${F}_clean
            cat /tmp/headers/${F}_header ${WD}/${F}_clean > /tmp/refs/${F}_r
          else
            cp $R /tmp/refs/${F}_r
          fi
        volumeMounts:
          - name: workdir
            mountPath: /tmp
        resources:
          requests:
            cpu: "1"
            memory: 800Mi
      retryStrategy: {}

    - name: prepareQuery
      inputs:
        parameters:
          - name: index
      script:
        image: >-
          harbor.computational.bio.uni-giessen.de/docker_hub_cache/library/busybox:1.35
        command: [sh]
        source: |
          mkdir -p /tmp/queries
          mkdir /tmp/query-dir-{{inputs.parameters.index}}
          WD="/tmp/query-dir-{{inputs.parameters.index}}"
          Q="/tmp/data/{{inputs.parameters.index}}.fas"
          F=$(basename $Q)
          cd $WD
          cat $Q | awk '/^>/ {x="split_"++i;next} {print > x}'
          for s in split_*; do
            cat $s | split -b 1020 -a 9 - fragment_
          done
          for frag in fragment_*; do
            (cat /tmp/headers/${F}_header $frag; echo) >> /tmp/queries/${F}_q
          done
        volumeMounts:
          - name: workdir
            mountPath: /tmp
        resources:
          requests:
            cpu: "1"
            memory: 800Mi
      retryStrategy: {}

    - name: calculate
      inputs:
        parameters:
          - name: index
      script:
        image: >-
          quay.io/biocontainers/blast:2.12.0--h3289130_3
        command: [bash]
        source: |
          mkdir -p /tmp/results
          mkdir /tmp/wd_pod-{{inputs.parameters.index}}

          LINES=($(cat /tmp/chunks/chunk_{{inputs.parameters.index}} | sed -e 's/),/;/g' -e 's/\s//g' | tr -d '][)(' | tr -d \'))

          readarray -d ';' -t combinations <<< $LINES

          for i in "${combinations[@]}"; do
            IFS=',' read Q R <<< $i
            
            mkdir /tmp/wd_pod-{{inputs.parameters.index}}/workdir
            WD="/tmp/wd_pod-{{inputs.parameters.index}}/workdir"

            A=$(basename $Q)
            B=$(basename $R)

            cd $WD

            blastn -query /tmp/queries/${A}_q -subject /tmp/refs/${B}_r \
              -xdrop_gap_final 150 -reward 1 -penalty -1 -dust no -gapopen 1 -gapextend 2 \
              -perc_identity 30 -evalue  0.000000000000001 -outfmt '6 qseqid sseqid pident length' \
              -out blast_1.out

            blastn -query /tmp/queries/${B}_q -subject /tmp/refs/${A}_r \
              -xdrop_gap_final 150 -reward 1 -penalty -1 -dust no -gapopen 1 -gapextend 2 \
              -perc_identity 30 -evalue  0.000000000000001 -outfmt '6 qseqid sseqid pident length' \
              -out blast_2.out

            if [ -s blast_1.out ]; then
              cat blast_1.out | awk ' $4 >= 714 { x += $3 ; y++ } END { print x / y }' > tmp_calc_1
            else
              echo 0 > tmp_calc_1
            fi
            if [ -s blast_2.out ]; then
              cat blast_2.out | awk ' $4 >= 714 { x += $3 ; y++ } END { print x / y }' > tmp_calc_2
            else
              echo 0 > tmp_calc_2
            fi

            cat /tmp/seqids/${A}_seqid /tmp/seqids/${B}_seqid tmp_calc_1 tmp_calc_2 | tr -s '\n' '\t' | xargs printf "%s,%s,%g,%g\n" >> /tmp/results/podresult_{{inputs.parameters.index}}.csv

            cd /tmp/wd_pod-{{inputs.parameters.index}} 

            rm -rf $WD 
          done
        volumeMounts:
          - name: workdir
            mountPath: /tmp
        resources:
          requests:
            cpu: "1"
            memory: 800Mi
      retryStrategy: {}

    - name: collect
      script:
        image: >-
          harbor.computational.bio.uni-giessen.de/docker_hub_cache/library/busybox:1.35
        command: [sh]
        source: |
          cat $(ls /tmp/results/podresult*) > /tmp/endresult_{{workflow.creationTimestamp}}.csv
        volumeMounts:
          - name: workdir
            mountPath: /tmp
        resources:
          requests:
            cpu: "1"
            memory: 6Gi
      outputs:
        artifacts:
          - name: resultCSV
            path: /tmp/endresult_{{workflow.creationTimestamp}}.csv
            s3:
              key: res/output_{{workflow.parameters.id}}_csv.tar.gz # edit

    - name: convertToJSON
      script:
        image: >-
          harbor.computational.bio.uni-giessen.de/docker_hub_cache/library/python:3.8-alpine
        command: [python]
        source: |
          import json
          import csv
          import os
          csvfile = open("/tmp/endresult_{{workflow.creationTimestamp}}.csv", "r")
          jsonfile = open("/tmp/endresult_{{workflow.creationTimestamp}}.json", "w")
          fieldnames = ("genome1","genome2","ani1","ani2")
          reader = csv.DictReader( csvfile, fieldnames)
          jsonfile.write("[\n")
          for row in reader:
              json.dump(row, jsonfile)
              jsonfile.write(",\n")
          jsonfile.close()
          csvfile.close()
          with open("/tmp/endresult_{{workflow.creationTimestamp}}.json", "rb+") as b:
              b.seek(-2, os.SEEK_END)
              b.truncate()
          with open("/tmp/endresult_{{workflow.creationTimestamp}}.json", "a") as a:
              a.write("]")
        volumeMounts:
          - name: workdir
            mountPath: /tmp
        resources:
          requests:
            cpu: "1"
            memory: 6Gi
      outputs:
        artifacts:
          - name: resultJSON
            path: /tmp/endresult_{{workflow.creationTimestamp}}.json
            s3:
              key: res/output_{{workflow.parameters.id}}_json.tar.gz # edit
